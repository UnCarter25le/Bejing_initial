{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 早稻田醫書古文網址\n",
    "http://www.wul.waseda.ac.jp/kotenseki/html/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先爬取outside links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#要抓的页面连结(outside)\n",
    "http://www.wul.waseda.ac.jp/kotenseki/html/ya09/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "outside_link = []     #搜集outside連結的list\n",
    "\n",
    "URL = 'http://www.wul.waseda.ac.jp/kotenseki/html/ya09/index.html'\n",
    "\n",
    "res = requests.get(URL)\n",
    "res.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(res.text,'lxml').select('a')   #找到連結那一項\n",
    "\n",
    "link = ['http://www.wul.waseda.ac.jp/kotenseki/html/ya09/'+roww['href'].replace('./','') for roww in soup]\n",
    "       #y09的網址＋                           串列生成式，roww['href']可以叫出網址\n",
    "[outside_link.append(row) for row in link]\n",
    "                  #把每個網址加入[]中\n",
    "print(outside_link[699])    #試著印出看看\n",
    "print(len(outside_link))    #所有outside link的數量\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把不合格的連結刪除\n",
    "del outside_link[697]\n",
    "del outside_link[697]\n",
    "del outside_link[697]\n",
    "del outside_link[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#試著印出\n",
    "outside_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有效outside link連結數量\n",
    "len(outside_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取outside_link_696\n",
    "import json\n",
    "with open('outside_link_{}.json'.format(len(outside_link)),'w')as f:\n",
    "    json.dump(outside_link,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 接著爬取inside links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#处理每个outside的inside连结 （基本上是一對一）\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "t1 = time.time()\n",
    "inside_link = []\n",
    "\n",
    "#讀取outside links 的json檔案\n",
    "with open('./outside_link_696.json',encoding = 'utf-8')as F:\n",
    "    inn = F.read()\n",
    "    outt = json.loads(inn)\n",
    "#找到跟outside link一對一的inside links\n",
    "for link in outt:\n",
    "    res = requests.get(link)\n",
    "    time.sleep(3)\n",
    "    res.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(res.text,'lxml').select('#image > a')[0] \n",
    "    inside_link.append(str(soup).split('\"')[1])\n",
    "    time.sleep(3)\n",
    "    print(len(inside_link))   #紀錄目前進行進度\n",
    "t2 = time.time()\n",
    "\n",
    "\n",
    "print(t2-t1)\n",
    "# 轉存inside links 的json檔案出去\n",
    "with open('inside_link_{}.json'.format(len(inside_link)),'w') as f:\n",
    "    json.dump(inside_link,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查inside links樣子\n",
    "outt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抓取每個inside links 裡，所有書籍的連結（pdf）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抓取inside连结的pdf连结\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pdf_link = []\n",
    "\n",
    "#讀取inside links 的json檔案\n",
    "with open('./inside_link_696.json',encoding = 'utf-8')as F:\n",
    "    inn = F.read()\n",
    "    outt = json.loads(inn)\n",
    "\n",
    "for link in outt:\n",
    "    res = requests.get(link)\n",
    "    time.sleep(3)\n",
    "    res.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(res.text,'lxml').select('a')\n",
    "    final_link = [link+row['href'] for row in soup]\n",
    "    time.sleep(2)\n",
    "    #final_link 是裝著pdf links的list，接著要對這些連結作篩選，篩選結尾是\".pdf\"的連結出來。\n",
    "    for row in final_link:\n",
    "        if re.findall(\".+\\.pdf\",row) == []:\n",
    "            pass\n",
    "        else:\n",
    "            pdf_link.append(re.findall(\".+\\.pdf\",row))\n",
    "            time.sleep(2)\n",
    "            print(len(pdf_link))  #檢視完成的筆數\n",
    "#寫出pdf links的json檔案出去\n",
    "with open('pdf_link_{}.json'.format(len(pdf_link)),'w') as f:\n",
    "    json.dump(pdf_link,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#檢視pdf links狀況\n",
    "pdf_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#檢視pdf links狀況\n",
    "pdf_link[2398]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 開始爬取pdf文本置本地端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取pdf的連結\n",
    "import json\n",
    "with open('pdf_link_2399.json', encoding = 'utf-8')as f:\n",
    "    inn = f.read()\n",
    "    outt = json.loads(inn)\n",
    "outt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#製作每一個檔案的檔名；其中pdf連結不是String格式，所以要轉型。\n",
    "import re\n",
    "re.findall('\\w+',str(outt[523]))[-2:-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#存pdf檔案\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "#待存放已爬取pdf links的list\n",
    "pdf_link_pool = []\n",
    "\n",
    "t1 = time.time()\n",
    "i = 0 \n",
    "for pdf in outt:               #以pdf[0]才會得到純粹的連結\n",
    "    pdf_link_pool.append(pdf[0])\n",
    "    r = requests.get(pdf[0])\n",
    "    time.sleep(2)\n",
    "    # 將網頁預覽的pdf檔案寫出至本地端\n",
    "    with open('D://日本文本//Bejin_initial_copy//book_PDF/{}.pdf'.format(re.findall('\\w+',str(pdf))[-2:-1][0]),'wb') as f:\n",
    "        f.write(r.content)\n",
    "    time.sleep(5)\n",
    "    #檢視完成進度\n",
    "    print(len(pdf_link_pool))\n",
    "    time.sleep(3)\n",
    "    #將以爬取完畢的連結json檔案寫出\n",
    "    with open('D://日本文本//Bejin_initial_copy//pdf_link_pool/pdf_link_pool_{}.json'.format(len(pdf_link_pool)),'w')as F:\n",
    "        json.dump(pdf_link_pool,F)\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬蟲斷掉，接續爬蟲策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取pdf所有的連結\n",
    "import json\n",
    "with open('pdf_link_2399.json', encoding = 'utf-8')as f:\n",
    "    inn = f.read()\n",
    "    outt = json.loads(inn)\n",
    "outt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取已爬取的pdf連結\n",
    "import json\n",
    "with open('D://日本文本//Bejin_initial_copy//pdf_link_pool//pdf_link_pool_1731.json', encoding = 'utf-8')as f:\n",
    "    inn = f.read()\n",
    "    outt2 = json.loads(inn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#確認已爬取的連結數量\n",
    "len(outt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#發現，if迴圈設計不當，導致真正數量比較少。\n",
    "len(set(outt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先將來放已爬取連結的list清空，再將已經實際爬取到的連結放進去。\n",
    "pdf_link_pool = []\n",
    "pdf_link_pool = outt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#檢視狀況\n",
    "pdf_link_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outt是所有pdf連結的list（2399筆）、pdf_link_pool是已經爬取到的連結\n",
    "#這邊是在衡量，還有多少連結未爬取\n",
    "i=0\n",
    "for pdf in outt:\n",
    "    if pdf[0] in pdf_link_pool:\n",
    "        pass\n",
    "    else:\n",
    "        i+=1\n",
    "        print(i)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#繼續轉存pdf檔案任務(可以避開已經存過的)     \n",
    "#(pdf_link_pool_139.json   因為if設計不當，所以實際上又是從第一筆開始存)\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "with open('D://日本文本//Bejin_initial_copy//pdf_link_pool//pdf_link_pool_1731.json', encoding = 'utf-8')as f:\n",
    "    inn = f.read()\n",
    "    outt2 = json.loads(inn)\n",
    "\n",
    "# pdf_link_pool = []\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "for pdf in outt:          #這邊的if else就是判斷，如果已經爬過的就pass；否則就繼續爬取轉存\n",
    "    if pdf[0] in pdf_link_pool:\n",
    "        pass\n",
    "    else:\n",
    "        pdf_link_pool.append(pdf[0])\n",
    "        r = requests.get(pdf[0])\n",
    "        time.sleep(2)\n",
    "        with open('D://日本文本//Bejin_initial_copy//book_PDF/{}.pdf'.format(re.findall('\\w+',str(pdf))[-2:-1][0]),'wb') as f:\n",
    "            f.write(r.content)\n",
    "        time.sleep(5)\n",
    "        print(len(pdf_link_pool))\n",
    "        time.sleep(3)\n",
    "\n",
    "        #將已爬取的pdf_link_pool這個list寫出\n",
    "        with open('D://日本文本//Bejin_initial_copy//pdf_link_pool/pdf_link_pool_{}.json'.format(len(pdf_link_pool)),'w')as F:\n",
    "            json.dump(pdf_link_pool,F)\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2537 - 139 + 1 = 2399\n",
    "完成！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
